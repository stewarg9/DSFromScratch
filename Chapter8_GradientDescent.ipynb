{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science is typically about trying to find the best model for a certain situation. Usually, \"best\" will mean something like \"minimising the error of the model\" or \"maximising the likelihood of the data\". In other words, it will represent the solution to some sort of optimisation problem. \n",
    "\n",
    "This means we'll need to solve a number of iotimisation problems. And, in particular, we'll need to solve them from scratch. Our approach will be a technique called *gradient descent*, which lends itself pretty well to a from-scratch treatment. You might not find it super-exciting in and of itself, but it will enable us to do exciting things from here on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipynb.fs.defs.Chapter4_LinearAlgebra as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The idea behind gradient descent\n",
    "\n",
    "Suppose we have some function $f$ that takes as input a vector of real numbers and outputs a single real number. One simple function is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll frequently need to maximise or minimise such functions. That is, we need to find the input $v$ that produces the largest (or smallest) possible value. \n",
    "\n",
    "For functions ike ours, the $gradient$ (if you remember calculus, this is the vector of partial derivatives) gives the input direction in which the function most quickly increases. Accordingly, one approach to maximising a function is to pick a random starting point, compute the gradient, take a small step in the direction of the gradient (i.e. the direction that causes the function to increase most) and repeat with the new starting point. Similarly, you can try to minimize a function by taking small steps in the $opposite$ direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "If a function has a unique global minimum, this procedure is likely to find it. If a function has multiple (local) minima, this procedure might \"find\" the wrong one, in which case, you might need to rerun this procedure fro a variety of starting points. If a function has no minimum then it's possible the procedure might go on for ever. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Estimating the Gradient\n",
    "If $f$ is a function of one variable, its derivative at point $x$ measures how $f(x)$ changes when we make a very small change to x. It is defined as the limit of the difference quotients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as h approaches zero. \n",
    "\n",
    "(Many a would-be calculus student has been stymied by the mathematical definition of the limits. Here, we'll cheat and simply say ut means what you think it means.)\n",
    "\n",
    "The derivative is the slope of the tangent line at $(x, f(x))$, while the difference quotient is the slope of the not-quite-tangent line that runs through $(x+h, f(x+h))$. As $h$ gets smaller and smaller, the not-quite-tangent line gets closer and closer to the tangent line. \n",
    "\n",
    "For many functions it's easy to exactly calculate derivatives. For example, the quare function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has the derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def derivative(x):\n",
    "    return 2 * x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which you can check, if you are so inclined by explicitly computing the difference quotient and taking the limit. \n",
    "\n",
    "What if you couldn't (or didn't want to) find the gradient? Although we can't take limits in Python, we can estimate derivatives by evaluating the difference quotient for a very small $e$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18c320a7780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.0001)\n",
    "\n",
    "# plot to show they're basically the same..\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(-10, 10)\n",
    "actual_y = list(map(derivative,x))\n",
    "estimated_y = list(map(derivative_estimate, x))\n",
    "\n",
    "print(actual_y)\n",
    "plt.title(\"Actual derivatives vs estimates\")\n",
    "plt.plot(x, actual_y, 'rx', label='Actual')              # red x\n",
    "plt.plot(x, estimated_y, 'b+', label='Estimate')   # blue +\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $f$ is a function of many variables, it has many *partial derivatives*, each indicating how $f$ changes when we make small chanhes in just one of the input variables. \n",
    "\n",
    "We calculate its *i*th partial derivative by treating it as a function of just its *i*th variable, holding other variables as fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial derivative quotient of f at v\"\"\"\n",
    "    v = [v_j + (h if j ==1 else 0)         # add h to just the ith element of v\n",
    "        for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return (f(v) - f(v)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after which we can estimate the gradient the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f,v,i,h)\n",
    "           for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Note\n",
    "\n",
    "A major drawback to the \"estimate using difference quotients\" approach is that it's computationally expensive. If v has a length n, estimate_gradient has to evaluate f on 2n different inputs. If you're repeatedly estimating gradients, you're doing a whole lot of extra work.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the gradient\n",
    "\n",
    "It's easy to see that the $sum_of_squares$ function is smallest when its input $v$ is a vector of zeroes. But imagine we didn't know that. Let's use gradients to find the minimum among all three-dimensiona;l vectors. We'll just pick a randm starting point and then take tiny steps in the opposite direction of the gradient until we reach a point where the gradient is very small:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4664989081760563e-06, 1.7332494540880281e-06, -2.8887490901467167e-06]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def step (v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from V\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# Pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)   # calculate gradient at v\n",
    "    next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
    "    if la.distance(next_v, v) < tolerance:     # stop if we're converging\n",
    "        print(next_v)\n",
    "        break\n",
    "    v = next_v                              # otherwise repeat again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above always produces a v that's very close to zero. The smaller the tolerance, the closer it gets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Right Step Size\n",
    "\n",
    "Although the rationale for moving against the gradient is clear, how vfar to move is not. Indeed, choosing the right step size is more of an art than a science. Some popular options include: \n",
    "- Using a fixed size\n",
    "- Gradually shrinking the step size over time\n",
    "- At each step, choosing the step size that minimises the value of the objective function. \n",
    "\n",
    "The last sounds optimal but is, in practice a costly computation. We can approximate it by trying a variety of step sizes and choosing the one that results in the smallest value of the objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that certain step sizes will result in invalid inputs for our function. So we'll need to create a \"safe apply\" function that returns infinity (which should never be the minimum value of anything) for invalid inputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\" return a new function that is the same as f except it returns infinity\n",
    "    in any scenario that f produces an error \"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')    # The python representation of infinity\n",
    "        \n",
    "    return safe_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the general case, we'll have some $target_fn$  that we want to minimise and we also have its $gradient_fn$. For example, the $target_fn$ could represent errors in a model as a function of its parameters and we may want to find the vaue of the parameters that make the errors as small as possible. \n",
    "\n",
    "Furthermore, let's say we have somehow chosen a starting point for the paranmeters $theta_0$. Then we can implement gradient descent as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimise_batch(target_fn, gradient_fn, theta_0, tolerance=0.0000001):\n",
    "    \"\"\"use gradient descent to tind the theta that minimises the target function\"\"\"\n",
    "    \n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0      # set the starting value\n",
    "    target_fn = safe(target_fn)  # safe version of target function\n",
    "    value = target_fn(theta)     # value we're minbimising\n",
    "    \n",
    "    while True: \n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                           for step_size in step_sizes] \n",
    "        \n",
    "        # Choose the one that minimises the error function\n",
    "        next_theta = min(next_thetas, key=tagret_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we're converging\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called it $minimise_batch$ because, for each gradient step it looks across the entire dataset (because $target_fn$ returns the error on the whole dataset). In the next section, we'll see an alternative approach that only looks at one data point at a time. \n",
    "\n",
    "Sometimes we'll instead want to *maximise* a function, which we can do by minimising the negative value. this has a corresponding negative gradient: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximise_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimise_batch(negate(target_fn), \n",
    "                         negate_all(gradient_fn),\n",
    "                         theta_0,\n",
    "                         tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Gradient descent is often used to choose the parameters of a model in such a way to minimise the notional error. Using the previous approach, each gradient step requires a prediction and compute the gradient for the dataset as a whole, which means each step takes a long time. \n",
    "\n",
    "Now, usually, these error functions are *additive*,meaning the predictive error on the whole dataset is simply the sum of the predictive errors for each point. \n",
    "\n",
    "When this is the case, we can instead apply a technique called *stochastic gradient descent* which computes the gradient and takes a step for only one poin at a time. It cycles over the data repeadedly until it reaches a stopping point. \n",
    "\n",
    "During each cycle, we'll iterate through the data set in a random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"generator that redturns elements from a data set in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]    # create a list of indexes\n",
    "    random.shuffle(indexes)                      # shuffle the indexes\n",
    "    for i in indexes:                            # return data in that randomised order\n",
    "        yield data[i]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, we'll want to take a gradient step for each data point. This approach leaves the possibility that we might circle around near a minimum forever, so whenever we stop getting improvements, we'll decrease the step size, then eventually stop iterating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minimise_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0 = 0.01):\n",
    "    \n",
    "    data = zip(x,y)\n",
    "    theta = theta_0                   # initial guess\n",
    "    alpha = alpha_0                   # initial step size\n",
    "    min_theta, min_value = None, float('inf')  # store current minimum\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # if we go over 100 iterations with no improvement, stop. \n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "        \n",
    "        if value <min_value:\n",
    "            # found a new minimum- store it \n",
    "            # use the original step size. \n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "            \n",
    "        else:\n",
    "            # otherwise, not improving, so shrink step size. \n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        for x_i, y_i in random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = la.vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic functiona will typically be much faster than the batch version. Of cource, we'll need a maximise function, too.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximise_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimise_stochastic(negate(target_fn), \n",
    "                              negate_all(gradient_fn), \n",
    "                              x, y, theta_0, alpha_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
