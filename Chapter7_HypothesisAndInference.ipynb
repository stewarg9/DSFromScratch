{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Hypothesis and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Hypothesis Testing\n",
    "\n",
    "Hypothesis are assertions like \"This coin is fair\", \"Data Scientists prefer python to R\", \"Popups dissuade visitors to our site\", etc, which can be translated into statistics about data. Under various assumptions, those statistics can be thought of as observations of random variables from known distributions, which allows us to make statements about how likely those assumtpions to hold. \n",
    "\n",
    "In the classic setup, we have a null hypothesis ${H_0}$, that represents some default position and some alternative hypothesis ${H_1}$ that we'd like to compare it with. We use statistcs to decide whether we can reject ${H_0}$ as false or not. \n",
    "\n",
    "### Example- Flipping a Coin. \n",
    "\n",
    "Imagine we have a coin, and we want to test whether it's fair. We make the assumption that the coin has some probability $p$ of landing on heads, so our null hypothesis is that the coin is fair- that $$p = 0.5$$. We'll test this agains the alternative hypothesis $$p \\neq 0.5$$. \n",
    "\n",
    "In particular, our test will involve flipping the coin some number $n$ times and counting the number of heads. Each coin flip is a Bernoulli trial, which means that X is a Binomial(n,p) random variable which(as we saw in Chapeter 6), we can approximate using the normal distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import ipynb.fs.defs.Chapter6_Probability as prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_approximation_to_binomial(n,p):\n",
    "    \"\"\"finds mu and sigma corresponding to a Binomial (n,p)\"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the normal_cdf function to figure out the probability that a random variable (following a normal distriubution) lies inside or outside a particular interval. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the normal cdf is the probability the variable is below a threshold\n",
    "normal_probability_below = prob.normal_cdf          # function synonym\n",
    "\n",
    "# Considered above the threshold if it's not below: \n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1- prob.normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# It's between if it's less than hi but not less than lo. \n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return prob.normal_cdf(hi, mu, sigma) - prob.normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# It's outside if it's not between\n",
    "def normal_probability_outside(lo, hi, mu, sigma):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also do the reverse- find the non-tail region or the symmetric interval around the mean that accounts for a certain level of likelihood. For example, if we want to find an interval centred at the mean and containing 60% probability, then we find the cutoffs where the upper and lower tail each contain 20% of the probability (thereby leaving 60%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"return the z for which P( Z <= z ) == probability\"\"\"\n",
    "    return prob.inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for which P( Z >= z ) == probability\"\"\"\n",
    "    return prob.inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\"return the symmetric (about the mean) bounds \n",
    "       that contain the specified probability \"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    \n",
    "    # Upper bound probability should have tail probability above it. \n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    # Lower bound probability should have tail probability above it. \n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we wanted to flip the coin 1000 times. If the coin was fair, X should be approximately normal with mean 500 and standard deviation 15.8. \n",
    "\n",
    "We need to make a decision about significance- how willing we are to make a type 1 error (false positive classification), in which we reject ${H_0}$ even though it is actually true. For reasons lost to time, the significance level is usually either 5% (1 in 20 type 1 error) or 1% (1 in 100 type 1 error). Choosing 5%: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(469.01026640487555, 530.9897335951244)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_0, sigma_0 = normal_approximation_to_binomial( 1000, 0.5)\n",
    "\n",
    "# Determine the test that rejects H0 if X falls outside the bounds below: \n",
    "\n",
    "normal_two_sided_bounds( 0.95, mu_0, sigma_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming p really equals 0.5 (i.e. ${H_0}$ is true), there is just a 5% chance we observe an X that lies outside this interval, which is the exact significance we wanted.  Said differently, if ${H_0}$ is true then, approximately 19 times out of 20, this test will give the correct result. \n",
    "\n",
    "We're often interested in the power of a test, which is the probability of not making a type 2 error, in which we fail to reject ${H_0}$ even though it is really false. In order to measure this, we have to specify what exactly ${H_0}$ being false means- knowing that p is not 0.5 doesn't say anything abpout the distribution of X. In particular, lets check what happens if p is really 0.55, so the coin has a slight bias towards heads. \n",
    "\n",
    "The power of the test can be calculated as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469.01026640487555 530.9897335951244\n",
      "550.0 15.732132722552274\n",
      "0.8865480012953671\n"
     ]
    }
   ],
   "source": [
    "# 95% bounds based on assumption p is 0.5. \n",
    "lo, hi = normal_two_sided_bounds(0.95, mu_0, sigma_0)\n",
    "print(lo, hi)\n",
    "\n",
    "# actual mu and sigma based on p=0.55\n",
    "mu_1, sigma_1 = normal_approximation_to_binomial(1000, 0.55)\n",
    "\n",
    "print(mu_1, sigma_1)\n",
    "\n",
    "# A type 2 error means we fail to reject the null hypothesis (H0)\n",
    "# which will happen when X is still in our original interval. \n",
    "type_2_probability = normal_probability_between(lo, hi, mu_1, sigma_1)\n",
    "\n",
    "power = 1 - type_2_probability\n",
    "\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine instead that our null hypothesis ${H_0}$ was that the coin is not biased towards heads, or that $p \\leq 0.5$. In that case, we want a one-sided test that rejects the null hypotheses when X is much larger than 500 but not when X is smaller than 500. So, a 5% significance test involves using normal_probability_below to find the cutoff below which 95% of the probabilities lie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.0073585242053\n",
      "0.9363794803307173\n"
     ]
    }
   ],
   "source": [
    "hi= normal_upper_bound(0.95, mu_0, sigma_0)\n",
    "print(hi)\n",
    "\n",
    "type_2_probability = normal_probability_below(hi, mu_1, sigma_1)\n",
    "power = 1 - type_2_probability\n",
    "print(power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values provide a different way to think about this. Instead of looking at broundaries based on a probability cutoff, we instead compute the probability (assuming ${H_0}$ is true) that we would see a value at least as extreme as the one we actually observed. \n",
    "\n",
    "For our two-sided test of whether the coin is fair, we could compute: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06207721579598857"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu: \n",
    "        \n",
    "        # if x is greater than the mean, the tail is what's greater than X...\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        # x is less than the mean, the tail is what's below x. \n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "    \n",
    "# e.g. p value for seeing 530 heads\n",
    "two_sided_p_value(529.5, mu_0, sigma_0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "The change from 530 to 529.5 is what's called a continuity correction. \n",
    "It reflects the fact that normal_probability_between(529.5, 350.5, mu_0, sigma_0) is a better estimate of the probability of seeing 530 heads than norma_probability_between(530,531,mu_0, sigma_0). \n",
    "\n",
    "Correspondingly, normal_probability_above(529.5, mu_0, sigma_0) is a better estimate of the probability of seeing at least 530 heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06155\n"
     ]
    }
   ],
   "source": [
    "# Simulation of this... \n",
    "\n",
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0    # count of heads\n",
    "                   for _ in range(1000))                 # in 1000 flips\n",
    "    if num_heads >= 530 or num_heads <= 470:              # count how often\n",
    "        extreme_value_count += 1                         # we see extremes\n",
    "        \n",
    "print(extreme_value_count / 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Warning!\n",
    "\n",
    "Make sre your data is roughly normally distributed before using normal_probability_above to compute p-values. Most predictive algorithms assume the target variable is normally distributed- if it's not, you're on shaky ground. \n",
    "\n",
    "there are various statistical tests to determine whether a variable is normally distributed, but even plotting the data is a good start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Intervals\n",
    "\n",
    "We have been testing hypotheses about the value of the heads probability p, which is the parameter of the unknown \"heads\" distribution. When this is the case, a third approach is to construct a confidence interval around the observed value of the parameter. \n",
    "\n",
    "For example, we can estimate the probability of the unfair coin by looking at the average value of the Bernoulli variables corresponding to each flip- 1 if heds, 0 if tails. If we observe 525 heads out of 1000 flips, then we estimate p equals 0.525. \n",
    "\n",
    "How confident can we be about this estimate? Well. if we knew the exact value of p, the central limit theorem tells us that the average of those Bernoulli variables should be approximately normal with mean p and standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b61fda1f50dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "math.sqrt(p * (1 - P) /1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we don't know p, so instead we use our estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015791611697353755\n"
     ]
    }
   ],
   "source": [
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "\n",
    "sigma= math.sqrt(p_hat * (1-p_hat)/ 1000)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not entirely justified, but people seem to do it anyway. Using the normal approximation, we conclude that we are \"95%\" confident that the following interval contains the true parameter p. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4940490278129096, 0.5559509721870904)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note\n",
    "This is a statement abot the interval- not the parameter p. You should read it as the assertion that if you were to repeat the experiment many times, 95% of the time the \"true\" parameter (which is the same every time) would lie within the observed confidence interval, which may be different every time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we do not conclude that the coin is unfair, since 0.5 falls within our confidence interval. \n",
    "If instead, we'd seen 540 heads, then we'd have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5091095927295919, 0.5708904072704082)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_hat = 540 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1-p_hat) / 1000)\n",
    "\n",
    "normal_two_sided_bounds(0.95, mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fair coin parameter (p = 0.5) doesn't lie within this interval. The fair coin hypothesis doesn't pass a test that you'd expect it to pass 95% of the time if it were true. '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Hacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A procedure that erroneously rejects the null hypothesis only 5% of the time will (by definition) 5% of the time erroneously reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"flip a fair coin 1000 times. True = heads, false = tails.\"\"\"\n",
    "    return [random.random() < 0.5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    \"\"\" using the 5% significance levels\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "random.seed(0)\n",
    "experiments = [run_experiment() for _ in range(1000)]\n",
    "num_rejections = len([experiment\n",
    "                     for experiment in experiments\n",
    "                     if reject_fairness(experiment)])\n",
    "\n",
    "print(num_rejections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is, if you're setting out to find \"significant\" results, you usually can. Test enough hypotheses against your data set and one of them will almost certainly appear significant. Remove the right outliers and you can probably get your p-value below 0.05. \n",
    "This is sometimes called p-hacking and is in some ways a consequence of the \"inference from p-values framework\". \n",
    "There are issues with this approach- see article \"The earth is round\". \n",
    "\n",
    "If you want to do good science, you should determine your hypothesis before looking at the datam you should clean the data without the hypotheses in mind and you should keep in mind that p-values are not a substitute for common sense. \n",
    "An alternative approach is Bayesian Inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Running an A/B Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of your primary responsibilities at DataSciencester is experience optimisation, which is a euphemism for trying to get people to click on adverts. \n",
    "One of your advertisers has developed a new energy drink targeted at data scientists and the VP of Advertisments wants your help choosing between advertisment A (\"tastes great!\") and advertisment B (\"less bias!\"). \n",
    "\n",
    "Being a scientist, you decide to run an experiment by randomly showing each site visitor one of the two advertisments and tracking how many people click on each one. \n",
    "\n",
    "If 990 out of 1000 A-viewers click their add while only 10 out of 1000 B-viewers click their ad, you can be pretty confident that A is the better ad. But, what if the differences are not so stark? Here's where you'd use statistical inference. \n",
    "\n",
    "Lets say that $N_A$ people see ad A, and that $n_A$ of them click it. We can think of each Bernoulli trial where $p_A$ is the probability that someonbe clicks ad A. Then, if $N_A$ is large, which it is here, we know that $n_A / N_A$ is approximately a normal random variable with mean $p_A$ and standard deviation $\\sigma_A = \\sqrt{p_A(1-p_A) / N_A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, $n_B / N/B$ is approximately a normal random variable with mean $p_B$ and standard deviation $\\sigma_B = \\sqrt{p_B (1 - p_B) / N_B}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimated_parameters(N,n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt( ( p * (1.0 - p) / N) )\n",
    "    return p, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume those two normals are independent (which would seem reasonable, since the individual Bernoulli trials should be), then their difference should also be normal with mean $p_B - p_A$ and standard deviation $\\sqrt{\\sigma^2_A + \\sigma^2_B}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "This is a type of cheating. The maths only works out exactly like this if you know the standard deviations. HEre, we're estimating them from the data, which means we really should be using the t-distribution. But, for a large enough data set, it's close enough that it doesn't make much difference. \n",
    "\n",
    "This means we can test the *null hypothesis* that $p_A $ and $p_B$ are the same (that is, $p_A - p_B$ is zero using the statistic: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    \n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B - p_A) / math.sqrt(pow(sigma_A,2.0) + pow(sigma_B, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should approximately be a standard nbormal variable. \n",
    "For example, if \"tastes great\" gets 200 clicks from 1000 views and \"less bias\" gets 180 clicks out of 1000 views, the statistic is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1403464899034472\n"
     ]
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 180)\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of seeing such a large difference if the means were actually equal would be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.254141976542236"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is large enough that you can't conclude there is much of a difference. On the other hand, if \"less bias\" got 150 clicks, we'd have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003189699706216853"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = a_b_test_statistic(1000, 200, 1000, 150)\n",
    "two_sided_p_value(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The procedures we've looked at have involved making probability statements about our tests- \"there's only a 4% chance you'd observe such an extreme statistic if our null hypothesis were true. \n",
    "\n",
    "An alternative apporach to inference involves treating the unknown parameters themselves as random variables. The analyst (that's you) starts with a *prior distribution* for the parameters and then use the observed data and Bayes Theorem to get an updated *posterior distribution* for the parameters. Rather than making probability judgments about the tests, you make probability judgments about the parameters themselves. \n",
    "\n",
    "For example, when the unknown parameter is a probability (as in our coin flipping example), we often use a prior from the *beta distribution*, which puts all its probability between 0 and 1. \n",
    "\n",
    "                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def B(alpha, beta):\n",
    "    \"\"\"a normalising constant so that the total probability is 1\"\"\"\n",
    "    return math.gamma(alpha) * math.gamma(beta) / math.gamma(alpha + beta)\n",
    "\n",
    "def beta_pdf(x, alpha, beta):\n",
    "    if x <= 0 or x >= 1:           # no weight outside of [0,1]\n",
    "        return 0\n",
    "    return x ** (alpha - 1) * (1 - x) ** (beta - 1) / B(alpha, beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, this distribution centres its weight at: \n",
    "    \n",
    "$alpha / (alpha + beta)$\n",
    "\n",
    "and the larger alpha and beta are, the tighter the distribution is. \n",
    "\n",
    "For example, if alpha and beta are both 1, it's just the uniform distribution (centred at 0.5, very dispersed) If alpha is much larger than beta, most of the weight is near 1. And if alpha is much smaller than beta, most of the weight is near zero. \n",
    "\n",
    "So, lets say we assume a prior distribution on $p$. Maybe we don;t want to take a stand on whether the coin is fair and we choose alpha and beta to both equal 1. Or maybe we have a strong belief that it lands heads 55% of the time and we choose alpha equals 44, beta equals 45. \n",
    "\n",
    "Then we flip our coin a bunch of times and see $h$ heads and $t$ tails. Bayes Theorem (and some mathematics that's too tedious to go through here) tells us tht the posterior distribution for p is again a Beta distribution but with parameters alpha + h and beta + t. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "It is no coincidence that the posterior distribution was again a Beta distribution. The number of heads is given by a Binomial distribution and the Beta is the *conjugate prior* to the Binomial distribution. This means that whenever you update a Beta prior using observations from the corresponding binomial, you will get back a Beta posterior. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say you flip the coin 10 times and see only 3 heads. \n",
    "\n",
    "If you started with the uniform prior (in some sense refusing to take a stand about the coin's fairness), your posterior distribution would be Beta(4,8) centres around 0.33. Since you considered all probabilities equally likely, your best guess is something pretty close to the obserbed probability. \n",
    "\n",
    "If you started with a Beta(20,20) expressing the belief that the coin was approximately fair, your posterior distribution would be a Beta(23,27), centred around 0.46 indicating a revised belief that maybe the coin is slightly biased towards tails. \n",
    "\n",
    "And if you started with a Beta(30,10) expressing a belief that the coin was biased to flip 75% heads, your posterior distribution would be a Beta(33,17) cenred around 0.66. In that case you'd still believe in a heads bias but less strongly than you did initially. \n",
    "\n",
    "If you flipped the coin more and more times, the prior would matter less and less until eventually you'd have (nearly) the same posterior distribution no mateer which prior you started with. \n",
    "\n",
    "For example, no matter how biased you initially thought the coin was, it would be hard to maintain that belief after seeing 1000 heads out of 2000 flips (unless you are a lunatic who picks something like a Beta(1000000,1) prior. \n",
    "\n",
    "What's interesting is that this allows us to make probability statements about hypotheses: \"Based on the prior and observed data, there is only a 5% likelihood he coin's heads probability is between 49% and 51%\". \n",
    "\n",
    "This is philosophically different from a statement like: \"If the coin were fair, we'd expect to observe data so extreme only 5% of the time\". \n",
    "\n",
    "Using Bayesian inference to test hypotheses is considered somewhat controversial, in part because its mathematics can get somewhat complicated and in part because  of the subjective nature of choosing a prior. We won't cover it any further, but it's good to be aware of. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
